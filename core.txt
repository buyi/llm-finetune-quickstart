âº ğŸ§  äºŒè¿›åˆ¶æ¨¡å‹æ–‡ä»¶åŠ è½½æ ¸å¿ƒåŸç†

  1. åŠ è½½ä»£ç 

  from transformers import AutoModelForCausalLM
  model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B")

  2. åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆ

  Step 1: è§£æäºŒè¿›åˆ¶æ–‡ä»¶

  # safetensorsæ ¼å¼ (988MBæ–‡ä»¶)
  import safetensors
  tensors = safetensors.torch.load_file("model.safetensors")

  # ç»“æœç±»ä¼¼ï¼š
  {
    "transformer.wte.weight": tensor([[0.1, 0.2, ...], ...]),  # è¯åµŒå…¥çŸ©é˜µ
    "transformer.h.0.attn.c_attn.weight": tensor(...),         # ç¬¬0å±‚æ³¨æ„åŠ›æƒé‡
    "transformer.h.0.mlp.c_fc.weight": tensor(...),            # ç¬¬0å±‚MLPæƒé‡
    "transformer.h.1.attn.c_attn.weight": tensor(...),         # ç¬¬1å±‚...
    # ... é‡å¤å¾ˆå¤šå±‚
    "lm_head.weight": tensor(...),                             # è¾“å‡ºå±‚æƒé‡
  }

  Step 2: æ„å»ºç¥ç»ç½‘ç»œç»“æ„

  # æ ¹æ®config.jsonæ„å»ºç½‘ç»œæ¶æ„
  class Qwen2Model(nn.Module):
      def __init__(self, config):
          self.embed_tokens = nn.Embedding(vocab_size, hidden_size)
          self.layers = nn.ModuleList([
              Qwen2DecoderLayer(config) for _ in range(num_layers)  # 24å±‚
          ])
          self.norm = nn.LayerNorm(hidden_size)

  class Qwen2DecoderLayer(nn.Module):
      def __init__(self):
          self.self_attn = Qwen2Attention()     # è‡ªæ³¨æ„åŠ›æœºåˆ¶
          self.mlp = Qwen2MLP()                 # å‰é¦ˆç½‘ç»œ
          self.input_layernorm = nn.LayerNorm()
          self.post_attention_layernorm = nn.LayerNorm()

  Step 3: æƒé‡æ˜ å°„åˆ°ç½‘ç»œ

  # å°†äºŒè¿›åˆ¶æ•°æ®æ˜ å°„åˆ°ç½‘ç»œå‚æ•°
  for name, param in model.named_parameters():
      if name in tensors:
          param.data = tensors[name]  # ç›´æ¥èµ‹å€¼

  # ä¾‹å¦‚ï¼š
  model.transformer.wte.weight.data = tensors["transformer.wte.weight"]
  model.transformer.h[0].attn.c_attn.weight.data = tensors["transformer.h.0.attn.c_attn.weight"]

  3. æ¨ç†è¿‡ç¨‹

  å‰å‘ä¼ æ’­æ ¸å¿ƒ

  def forward(self, input_ids):
      # 1. è¯åµŒå…¥ï¼štoken â†’ å‘é‡
      x = self.embed_tokens(input_ids)  # [batch, seq_len] â†’ [batch, seq_len, 896]

      # 2. é€å±‚Transformer
      for layer in self.layers:  # 24å±‚
          x = layer(x)  # è‡ªæ³¨æ„åŠ› + MLP

      # 3. è¾“å‡ºå±‚ï¼šå‘é‡ â†’ è¯æ±‡æ¦‚ç‡
      logits = self.lm_head(x)  # [batch, seq_len, vocab_size]
      return logits

  å•å±‚Transformerè¯¦ç»†

  def transformer_layer(x):
      # è‡ªæ³¨æ„åŠ›ï¼šå­¦ä¹ è¯ä¸è¯ä¹‹é—´çš„å…³ç³»
      attn_output = self_attention(x)  # æŸ¥è¯¢å½“å‰è¯ä¸å†å²è¯çš„å…³è”
      x = x + attn_output  # æ®‹å·®è¿æ¥

      # MLPï¼šéçº¿æ€§å˜æ¢
      mlp_output = mlp(x)  # ä¸¤å±‚å…¨è¿æ¥ + æ¿€æ´»å‡½æ•°
      x = x + mlp_output   # æ®‹å·®è¿æ¥
      return x

  4. æ ¸å¿ƒæ•°æ®ç»“æ„

  æƒé‡çŸ©é˜µå°±æ˜¯æ•°å­—

  # 988MBæ–‡ä»¶æœ¬è´¨ä¸Šå°±æ˜¯å¤§é‡æµ®ç‚¹æ•°
  weights = {
      "layer0_attention": [
          [0.1, 0.2, 0.3, ...],  # ç¬¬1è¡Œæƒé‡
          [0.4, 0.5, 0.6, ...],  # ç¬¬2è¡Œæƒé‡
          # ... 896è¡Œ Ã— 896åˆ— = 802,816ä¸ªæ•°å­—
      ],
      "layer0_mlp": [...],  # æ›´å¤šæ•°å­—çŸ©é˜µ
      # 24å±‚ Ã— æ¯å±‚å¤šä¸ªçŸ©é˜µ = 5äº¿ä¸ªå‚æ•°
  }

  æ¨ç†å°±æ˜¯çŸ©é˜µè¿ç®—

  # æœ¬è´¨ï¼šè¾“å…¥ Ã— æƒé‡çŸ©é˜µ = è¾“å‡º
  input_vector = [0.1, 0.2, 0.3, ...]     # è¾“å…¥è¯çš„å‘é‡è¡¨ç¤º
  weight_matrix = [[...], [...], ...]      # ä»988MBæ–‡ä»¶åŠ è½½çš„æƒé‡
  output = input_vector @ weight_matrix     # çŸ©é˜µä¹˜æ³•

  ğŸ¯ æ€»ç»“

  988MBäºŒè¿›åˆ¶æ–‡ä»¶ = 5äº¿ä¸ªæµ®ç‚¹æ•° = ç¥ç»ç½‘ç»œçš„"å¤§è„‘"

  1. åŠ è½½: å°†äºŒè¿›åˆ¶æ•°æ®è§£æä¸ºæ•°å­—çŸ©é˜µ
  2. æ˜ å°„: æ•°å­—å¡«å…¥é¢„å®šä¹‰çš„ç½‘ç»œç»“æ„
  3. æ¨ç†: é€šè¿‡çŸ©é˜µè¿ç®—é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
  4. è¾“å‡º: æ¦‚ç‡åˆ†å¸ƒ â†’ é€‰æ‹©æœ€å¯èƒ½çš„è¯

  æ ¸å¿ƒ: æ‰€æœ‰çš„"æ™ºèƒ½"éƒ½æ¥è‡ªè¿™5äº¿ä¸ªç²¾å¿ƒè®­ç»ƒçš„æ•°å­—ï¼