#!/usr/bin/env python3
"""
ç®€åŒ–æ¨ç†è„šæœ¬ - å‘½ä»¤è¡Œäº¤äº’ï¼Œé¿å…Gradioä¾èµ–é—®é¢˜
"""

import os
import torch
import yaml
from typing import Optional
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time

class SimpleInferenceEngine:
    """ç®€å•æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path, base_model_name=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.tokenizer = self.load_model(model_path, base_model_name)
        
    def load_model(self, model_path, base_model_name=None):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
        
        if base_model_name and os.path.exists(model_path):
            # åŠ è½½åŸºç¡€æ¨¡å‹ + LoRA
            model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float32,
                device_map=None,
                trust_remote_code=True
            )
            model = PeftModel.from_pretrained(model, model_path)
            tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
            print("âœ… åŠ è½½å¾®è°ƒæ¨¡å‹ï¼ˆLoRAï¼‰")
        else:
            # ç›´æ¥åŠ è½½å®Œæ•´æ¨¡å‹æˆ–ä½¿ç”¨åŸºç¡€æ¨¡å‹
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float32,
                    device_map=None,
                    trust_remote_code=True
                )
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                print("âœ… åŠ è½½å®Œæ•´æ¨¡å‹")
            except:
                print("âš ï¸ æœªæ‰¾åˆ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None,
                    trust_remote_code=True
                )
                tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ä½¿ç”¨è®¾å¤‡: {self.device}")
        return model, tokenizer
    
    def generate(self, 
                prompt: str,
                max_new_tokens: int = 256,
                temperature: float = 0.7,
                top_p: float = 0.9,
                top_k: int = 50):
        """ç”Ÿæˆå“åº”"""
        
        # æ„å»ºè¾“å…¥
        formatted_prompt = f"### æŒ‡ä»¤ï¼š\n{prompt}\n\n### å›ç­”ï¼š\n"
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # ç”Ÿæˆ
        start_time = time.time()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response[len(formatted_prompt):].strip()
        
        generation_time = time.time() - start_time
        tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])
        
        return {
            'response': response,
            'generation_time': f"{generation_time:.2f}s",
            'tokens_generated': tokens_generated,
            'tokens_per_second': f"{tokens_generated/generation_time:.1f}"
        }

def interactive_chat(engine):
    """äº¤äº’å¼èŠå¤©"""
    print("\n" + "="*60)
    print("ğŸ¤– LLMå¾®è°ƒæ¨¡å‹æ¨ç†æ¼”ç¤º")
    print("="*60)
    print("ğŸ’¡ è¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("ğŸ’¡ è¾“å…¥ 'clear' æ¸…å±")
    print("ğŸ’¡ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("-"*60)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ™‹ ä½ : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif user_input.lower() in ['clear', 'æ¸…å±']:
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif user_input.lower() in ['help', 'å¸®åŠ©']:
                print("\nğŸ“š ä½¿ç”¨è¯´æ˜:")
                print("  - ç›´æ¥è¾“å…¥é—®é¢˜å³å¯")
                print("  - 'quit/exit' : é€€å‡ºç¨‹åº")
                print("  - 'clear' : æ¸…å±")
                print("  - 'help' : æ˜¾ç¤ºå¸®åŠ©")
                continue
            elif not user_input:
                continue
            
            # ç”Ÿæˆå›ç­”
            print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
            result = engine.generate(user_input)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ¤– åŠ©æ‰‹: {result['response']}")
            print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡: {result['generation_time']} | {result['tokens_generated']} tokens | {result['tokens_per_second']} tokens/s")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {e}")

def batch_test(engine):
    """æ‰¹é‡æµ‹è¯•"""
    test_cases = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—é˜¶ä¹˜çš„å‡½æ•°",
        "å°†è¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª æ‰¹é‡æµ‹è¯•æ¨¡å¼")
    print("="*60)
    
    for i, prompt in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_cases)}: {prompt}")
        print("-" * 40)
        
        result = engine.generate(prompt, max_new_tokens=150)
        print(f"ğŸ¤– å›ç­”: {result['response']}")
        print(f"ğŸ“Š ç»Ÿè®¡: {result['generation_time']} | {result['tokens_per_second']} tokens/s")

def main():
    import argparse
    parser = argparse.ArgumentParser(description="ç®€åŒ–LLMæ¨ç†å·¥å…·")
    parser.add_argument("--model-path", type=str, default="./outputs/qwen-lora",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base-model", type=str, default="Qwen/Qwen2-0.5B",
                       help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument("--batch", action="store_true",
                       help="æ‰¹é‡æµ‹è¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    print("ğŸš€ å¯åŠ¨æ¨ç†å¼•æ“...")
    engine = SimpleInferenceEngine(args.model_path, args.base_model)
    
    if args.batch:
        batch_test(engine)
    else:
        interactive_chat(engine)

if __name__ == "__main__":
    main()