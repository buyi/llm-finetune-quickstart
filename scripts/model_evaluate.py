#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬ - è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½
"""

import json
import torch
import yaml
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset
# import evaluate  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œé¿å…ä¾èµ–é—®é¢˜
from tqdm import tqdm
import numpy as np
from collections import Counter

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def load_model_and_tokenizer(model_path, base_model_name=None):
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    
    if base_model_name:
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        # åŠ è½½LoRAæƒé‡
        model = PeftModel.from_pretrained(model, model_path)
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    else:
        # ç›´æ¥åŠ è½½åˆå¹¶åçš„æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_length=512):
    """ç”Ÿæˆæ¨¡å‹å“åº”"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå–ç”Ÿæˆçš„éƒ¨åˆ†
    response = response[len(prompt):].strip()
    return response

def evaluate_perplexity(model, tokenizer, dataset, num_samples=100):
    """è®¡ç®—å›°æƒ‘åº¦"""
    print("\nğŸ“Š è®¡ç®—å›°æƒ‘åº¦...")
    
    total_loss = 0
    total_tokens = 0
    
    for i in tqdm(range(min(num_samples, len(dataset)))):
        text = dataset[i]['text']
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
            total_loss += loss.item() * inputs["input_ids"].size(1)
            total_tokens += inputs["input_ids"].size(1)
    
    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))
    return perplexity.item()

def evaluate_generation_quality(model, tokenizer, test_prompts):
    """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
    print("\nğŸ¯ è¯„ä¼°ç”Ÿæˆè´¨é‡...")
    results = []
    
    for prompt in tqdm(test_prompts):
        response = generate_response(model, tokenizer, prompt['instruction'])
        results.append({
            'instruction': prompt['instruction'],
            'generated': response,
            'expected': prompt.get('output', '')
        })
    
    return results

def calculate_metrics(results):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    print("\nğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # åŸºç¡€ç»Ÿè®¡
    total = len(results)
    avg_length = np.mean([len(r['generated'].split()) for r in results])
    
    # å“åº”å®Œæ•´æ€§ï¼ˆæ˜¯å¦ç”Ÿæˆäº†å†…å®¹ï¼‰
    non_empty = sum(1 for r in results if len(r['generated'].strip()) > 0)
    completeness_rate = non_empty / total if total > 0 else 0
    
    # å¤šæ ·æ€§è¯„ä¼°ï¼ˆunique n-gramsï¼‰
    all_tokens = []
    for r in results:
        all_tokens.extend(r['generated'].split())
    
    unique_unigrams = len(set(all_tokens))
    total_unigrams = len(all_tokens)
    unigram_diversity = unique_unigrams / total_unigrams if total_unigrams > 0 else 0
    
    # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è¯„ä¼°ï¼ˆå¦‚æœæœ‰å‚è€ƒç­”æ¡ˆï¼‰
    similarity_scores = []
    
    for r in results:
        if r['expected']:
            # ç®€å•çš„è¯æ±‡é‡å åº¦è®¡ç®—
            gen_words = set(r['generated'].lower().split())
            exp_words = set(r['expected'].lower().split())
            
            if len(exp_words) > 0:
                overlap = len(gen_words.intersection(exp_words))
                similarity = overlap / len(exp_words)
                similarity_scores.append(similarity)
    
    avg_similarity = np.mean(similarity_scores) if similarity_scores else 0
    
    metrics = {
        'total_samples': total,
        'completeness_rate': completeness_rate,
        'avg_response_length': avg_length,
        'unigram_diversity': unigram_diversity,
        'avg_similarity': avg_similarity
    }
    
    return metrics

def main():
    # åŠ è½½é…ç½®
    config = load_config('./configs/training_config.yaml')
    
    # æ¨¡å‹è·¯å¾„
    model_path = config['training']['output_dir']
    base_model = config['model']['name']
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(model_path, base_model)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
    eval_dataset = load_dataset(
        'json',
        data_files={'eval': './data/eval_formatted.jsonl'}
    )['eval']
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        {"instruction": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ", "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸ"},
        {"instruction": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—", "output": ""},
        {"instruction": "å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼šäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ", "output": ""},
        {"instruction": "æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼šå¤§è¯­è¨€æ¨¡å‹æ˜¯åŸºäºTransformeræ¶æ„çš„æ¨¡å‹", "output": ""},
        {"instruction": "åˆ¤æ–­æƒ…æ„Ÿå€¾å‘ï¼šè¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘å¾ˆå–œæ¬¢", "output": "æ­£é¢"},
    ]
    
    # è¯„ä¼°å›°æƒ‘åº¦
    perplexity = evaluate_perplexity(model, tokenizer, eval_dataset, num_samples=50)
    
    # è¯„ä¼°ç”Ÿæˆè´¨é‡
    generation_results = evaluate_generation_quality(model, tokenizer, test_prompts)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(generation_results)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*50)
    print(f"å›°æƒ‘åº¦ (Perplexity): {perplexity:.2f}")
    print(f"å®Œæˆç‡: {metrics['completeness_rate']*100:.1f}%")
    print(f"å¹³å‡å“åº”é•¿åº¦: {metrics['avg_response_length']:.1f} è¯")
    print(f"è¯æ±‡å¤šæ ·æ€§: {metrics['unigram_diversity']:.3f}")
    if metrics['avg_similarity'] > 0:
        print(f"æ–‡æœ¬ç›¸ä¼¼åº¦: {metrics['avg_similarity']:.3f}")
    
    print("\nğŸ“ ç”Ÿæˆç¤ºä¾‹:")
    print("-"*50)
    for i, result in enumerate(generation_results[:3], 1):
        print(f"\nç¤ºä¾‹ {i}:")
        print(f"æŒ‡ä»¤: {result['instruction']}")
        print(f"ç”Ÿæˆ: {result['generated'][:200]}...")
        if result['expected']:
            print(f"å‚è€ƒ: {result['expected'][:200]}...")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    output_dir = Path('./outputs/evaluation')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_dir / 'evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'metrics': metrics,
            'perplexity': perplexity,
            'generation_samples': generation_results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'evaluation_results.json'}")

if __name__ == "__main__":
    main()