#!/usr/bin/env python3
"""
å¾®è°ƒè®­ç»ƒè„šæœ¬ - ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ
"""

import os
import sys
import yaml
import torch
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def prepare_model_and_tokenizer(config):
    """å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒ
    use_cuda = torch.cuda.is_available()
    use_quantization = use_cuda and config['model']['load_in_4bit']
    
    print(f"   CUDAå¯ç”¨: {use_cuda}")
    print(f"   ä½¿ç”¨é‡åŒ–: {use_quantization}")
    
    # é‡åŒ–é…ç½®ï¼ˆä»…åœ¨CUDAå¯ç”¨æ—¶ï¼‰
    bnb_config = None
    if use_quantization:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True
        )
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        config['model']['name'],
        quantization_config=bnb_config,
        device_map="auto" if use_cuda else None,
        trust_remote_code=True,
        torch_dtype=torch.float16 if use_cuda else torch.float32
    )
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        config['model']['name'],
        trust_remote_code=True
    )
    
    # è®¾ç½®padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒ
    if use_quantization:
        model = prepare_model_for_kbit_training(model)
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=config['lora']['r'],
        lora_alpha=config['lora']['lora_alpha'],
        target_modules=config['lora']['target_modules'],
        lora_dropout=config['lora']['lora_dropout'],
        task_type=TaskType[config['lora']['task_type']],
        bias="none"
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model, tokenizer

def preprocess_function(examples, tokenizer, max_length):
    """é¢„å¤„ç†æ•°æ®"""
    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    result = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=max_length,
    )
    result["labels"] = result["input_ids"].copy()
    return result

def load_and_prepare_dataset(config, tokenizer):
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†"""
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    
    # åŠ è½½æ•°æ®
    dataset = load_dataset(
        'json',
        data_files={
            'train': config['data']['train_file'].replace('.jsonl', '_formatted.jsonl'),
            'eval': config['data']['eval_file'].replace('.jsonl', '_formatted.jsonl')
        }
    )
    
    # é¢„å¤„ç†æ•°æ®
    tokenized_dataset = dataset.map(
        lambda x: preprocess_function(x, tokenizer, config['data']['max_length']),
        batched=True,
        remove_columns=dataset["train"].column_names
    )
    
    return tokenized_dataset

def main():
    # åŠ è½½é…ç½®
    config = load_config('./configs/training_config.yaml')
    
    # å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = prepare_model_and_tokenizer(config)
    
    # å‡†å¤‡æ•°æ®é›†
    dataset = load_and_prepare_dataset(config, tokenizer)
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config['training']['output_dir'],
        num_train_epochs=config['training']['num_train_epochs'],
        per_device_train_batch_size=config['training']['per_device_train_batch_size'],
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
        learning_rate=float(config['training']['learning_rate']),
        warmup_steps=config['training']['warmup_steps'],
        logging_steps=config['training']['logging_steps'],
        save_steps=config['training']['save_steps'],
        eval_steps=config['training']['eval_steps'],
        save_total_limit=config['training']['save_total_limit'],
        load_best_model_at_end=config['training']['load_best_model_at_end'],
        eval_strategy=config['training']['evaluation_strategy'],
        fp16=config['training']['fp16'] and torch.cuda.is_available(),
        gradient_checkpointing=config['training']['gradient_checkpointing'],
        report_to=config['training']['report_to'],
        logging_dir=f"{config['training']['output_dir']}/logs",
        remove_unused_columns=False,
        dataloader_pin_memory=False,
    )
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["eval"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(dataset['train'])}")
    print(f"   éªŒè¯æ ·æœ¬æ•°: {len(dataset['eval'])}")
    print(f"   è®­ç»ƒè½®æ•°: {config['training']['num_train_epochs']}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config['training']['per_device_train_batch_size']}")
    print(f"   å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"   è¾“å‡ºç›®å½•: {config['training']['output_dir']}")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(config['training']['output_dir'])
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {config['training']['output_dir']}")
    
    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    if trainer.state.log_history:
        final_stats = trainer.state.log_history[-1]
        print("\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
        if 'loss' in final_stats:
            print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_stats['loss']:.4f}")
        if 'eval_loss' in final_stats:
            print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {final_stats['eval_loss']:.4f}")

if __name__ == "__main__":
    main()