#!/usr/bin/env python3
"""
æ¨ç†éƒ¨ç½²è„šæœ¬ - æä¾›Webç•Œé¢å’ŒAPIæœåŠ¡
"""

import os
import torch
import yaml
import json
import gradio as gr
from typing import Optional
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time

class InferenceEngine:
    """æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path, base_model_name=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.tokenizer = self.load_model(model_path, base_model_name)
        
    def load_model(self, model_path, base_model_name=None):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
        
        if base_model_name:
            # åŠ è½½åŸºç¡€æ¨¡å‹ + LoRA
            model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            model = PeftModel.from_pretrained(model, model_path)
            tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        else:
            # ç›´æ¥åŠ è½½å®Œæ•´æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ä½¿ç”¨è®¾å¤‡: {self.device}")
        return model, tokenizer
    
    def generate(self, 
                prompt: str,
                max_new_tokens: int = 256,
                temperature: float = 0.7,
                top_p: float = 0.9,
                top_k: int = 50,
                do_sample: bool = True):
        """ç”Ÿæˆå“åº”"""
        
        # æ„å»ºè¾“å…¥
        formatted_prompt = f"### æŒ‡ä»¤ï¼š\n{prompt}\n\n### å›ç­”ï¼š\n"
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        start_time = time.time()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response[len(formatted_prompt):].strip()
        
        generation_time = time.time() - start_time
        tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])
        
        return {
            'response': response,
            'generation_time': f"{generation_time:.2f}s",
            'tokens_generated': tokens_generated,
            'tokens_per_second': f"{tokens_generated/generation_time:.1f}"
        }

def create_gradio_interface(engine):
    """åˆ›å»ºGradio Webç•Œé¢"""
    
    def chat_function(message, history, temperature, max_tokens, top_p):
        """èŠå¤©å‡½æ•°"""
        result = engine.generate(
            prompt=message,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        response = result['response']
        stats = f"\n\n---\nâ±ï¸ ç”Ÿæˆæ—¶é—´: {result['generation_time']} | ğŸ“ Tokenæ•°: {result['tokens_generated']} | âš¡ é€Ÿåº¦: {result['tokens_per_second']} tokens/s"
        
        return response + stats
    
    # åˆ›å»ºç•Œé¢
    demo = gr.ChatInterface(
        fn=chat_function,
        title="ğŸ¤– LLMå¾®è°ƒæ¨¡å‹æ¨ç†æ¼”ç¤º",
        description="ä¸å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚å¯ä»¥è°ƒæ•´ç”Ÿæˆå‚æ•°æ¥æ§åˆ¶è¾“å‡ºè´¨é‡ã€‚",
        examples=[
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
            "å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½",
            "å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
            "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯",
        ],
        additional_inputs=[
            gr.Slider(minimum=0.1, maximum=2.0, value=0.7, step=0.1, label="Temperature (åˆ›é€ æ€§)"),
            gr.Slider(minimum=50, maximum=500, value=256, step=50, label="Max Tokens (æœ€å¤§é•¿åº¦)"),
            gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.1, label="Top-p (å¤šæ ·æ€§)"),
        ],
        theme=gr.themes.Soft(),
    )
    
    return demo

def create_simple_interface(engine):
    """åˆ›å»ºç®€å•ç•Œé¢"""
    
    def inference_function(
        prompt,
        temperature=0.7,
        max_tokens=256,
        top_p=0.9,
        top_k=50
    ):
        """æ¨ç†å‡½æ•°"""
        if not prompt:
            return "è¯·è¾“å…¥æç¤ºè¯", ""
        
        result = engine.generate(
            prompt=prompt,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k
        )
        
        stats = f"""
### ğŸ“Š ç”Ÿæˆç»Ÿè®¡
- â±ï¸ ç”Ÿæˆæ—¶é—´: {result['generation_time']}
- ğŸ“ ç”ŸæˆTokenæ•°: {result['tokens_generated']}
- âš¡ ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_second']} tokens/s
"""
        
        return result['response'], stats
    
    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="LLMæ¨ç†éƒ¨ç½²", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸš€ LLMå¾®è°ƒæ¨¡å‹æ¨ç†éƒ¨ç½²")
        gr.Markdown("è¾“å…¥æŒ‡ä»¤ï¼Œæ¨¡å‹å°†ç”Ÿæˆç›¸åº”çš„å›ç­”ã€‚")
        
        with gr.Row():
            with gr.Column(scale=6):
                prompt_input = gr.Textbox(
                    label="è¾“å…¥æŒ‡ä»¤",
                    placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                    lines=3
                )
                
                with gr.Row():
                    with gr.Column():
                        temperature = gr.Slider(
                            minimum=0.1, maximum=2.0, value=0.7, step=0.1,
                            label="Temperature (æ§åˆ¶åˆ›é€ æ€§)"
                        )
                        max_tokens = gr.Slider(
                            minimum=50, maximum=500, value=256, step=50,
                            label="Max Tokens (æœ€å¤§ç”Ÿæˆé•¿åº¦)"
                        )
                    
                    with gr.Column():
                        top_p = gr.Slider(
                            minimum=0.1, maximum=1.0, value=0.9, step=0.1,
                            label="Top-p (æ ¸é‡‡æ ·)"
                        )
                        top_k = gr.Slider(
                            minimum=10, maximum=100, value=50, step=10,
                            label="Top-k (å€™é€‰è¯æ•°é‡)"
                        )
                
                submit_btn = gr.Button("ğŸ¯ ç”Ÿæˆå›ç­”", variant="primary")
                
            with gr.Column(scale=6):
                output_text = gr.Textbox(
                    label="æ¨¡å‹è¾“å‡º",
                    lines=10
                )
                stats_text = gr.Markdown(label="ç”Ÿæˆç»Ÿè®¡")
        
        gr.Examples(
            examples=[
                ["ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"],
                ["ç”¨Pythonå®ç°å†’æ³¡æ’åº"],
                ["å†™ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„çŸ­æ•…äº‹"],
                ["è§£é‡Šä»€ä¹ˆæ˜¯åŒºå—é“¾"],
                ["å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"],
            ],
            inputs=prompt_input
        )
        
        submit_btn.click(
            fn=inference_function,
            inputs=[prompt_input, temperature, max_tokens, top_p, top_k],
            outputs=[output_text, stats_text]
        )
    
    return demo

def main():
    import argparse
    parser = argparse.ArgumentParser(description="LLMæ¨ç†éƒ¨ç½²")
    parser.add_argument("--model-path", type=str, default="./outputs/qwen-lora",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base-model", type=str, default="Qwen/Qwen2-0.5B",
                       help="åŸºç¡€æ¨¡å‹åç§°ï¼ˆå¦‚æœä½¿ç”¨LoRAï¼‰")
    parser.add_argument("--port", type=int, default=7860,
                       help="æœåŠ¡ç«¯å£")
    parser.add_argument("--share", action="store_true",
                       help="ç”Ÿæˆå…¬å…±é“¾æ¥")
    parser.add_argument("--chat", action="store_true",
                       help="ä½¿ç”¨èŠå¤©ç•Œé¢")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    print("ğŸš€ å¯åŠ¨æ¨ç†æœåŠ¡...")
    engine = InferenceEngine(args.model_path, args.base_model)
    
    # åˆ›å»ºç•Œé¢
    if args.chat:
        demo = create_gradio_interface(engine)
    else:
        demo = create_simple_interface(engine)
    
    # å¯åŠ¨æœåŠ¡
    print(f"\nâœ… æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print(f"   æœ¬åœ°è®¿é—®: http://localhost:{args.port}")
    if args.share:
        print(f"   ç”Ÿæˆå…¬å…±é“¾æ¥ä¸­...")
    
    demo.launch(
        server_port=args.port,
        share=args.share,
        server_name="0.0.0.0"
    )

if __name__ == "__main__":
    main()